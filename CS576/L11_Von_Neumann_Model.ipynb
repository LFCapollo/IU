{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Distributed and Parallel Computing (CS576)**\n",
    "<br>\n",
    "Date: **26 February 2020**\n",
    "<br>\n",
    "Location: **SU, NEW STEM building**\n",
    "<br>\n",
    "Room: **302**\n",
    "<br>\n",
    "Week 1\n",
    "<br>\n",
    "Title: **Introduction to Parallel Programming**\n",
    "<br>\n",
    "Speaker: **Dr. Shota Tsiskaridze**\n",
    "<br>\n",
    "Bibliography: [1] Peter S. Pacheco, *An Introduction to Parallel Programming*, Morgan Kaufmann, 2011.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Introduction to Parallel Computing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Why Parallel Computing?</h3>\n",
    "\n",
    "**1) Aren’t single processor systems fast enough?**\n",
    "\n",
    "As the computational power increases, the number of problems that can be solved also increases.\n",
    "<br>\n",
    "Also, by some estimates, the quantity of data stored worldwide **doubles** every two years!\n",
    "\n",
    "**2) Why can’t microprocessor manufacturers continue to develop much faster single processor systems?**\n",
    "\n",
    "Air-cooled integrated circuits are reaching the limits of their ability to dissipate heat.\n",
    "\n",
    "**3) Why build parallel systems and systems with multiple processors?**\n",
    "\n",
    "If the industry doesn’t continue to bring out new products, it will effectively cease to exist.\n",
    "**4) Why do not write programs that will automatically convert $serial$ $programs$ into $parallel$ $programs$?**\n",
    "\n",
    "The bad news is that researchers have had very limited success writing the $translation$ $program$ that convert serial programs into parallel programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Moore's Law</h3>\n",
    "\n",
    "- From 1986 to 2002 the performance of microprocessors increased, on average, 50% per year.\n",
    "- Since 2002 single-processor performance improvement has slowed to about 20% per year.\n",
    "- The difference in performance increase is related with the multiprocessor systems.\n",
    "  <img src=\"images/001_Moores_Law.jpg\" width=\"600\" height=\"600\" alt=\"Moores Law\"  align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example 1</h3>\n",
    "\n",
    "Suppose that we need to compute $n$ values and add them together. For example $n = 24$:\n",
    "\n",
    "$$1, 4, 3, 9, 2, 8, 5, 1, 1, 6, 2, 7, 2, 5, 0, 4, 1, 8, 6, 5, 1, 2, 3, 9.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We know that this can be done with the following serial code (Pseudo-code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i in range(0, n):\n",
    "    x = \"Compute next value\"\n",
    "    sum += x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, if we have $p=8$ cores, then each core can sum of approximately $n/p=3$ values:\n",
    "\n",
    "|  Core  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "|--------|---|---|---|---|---|---|---|---|\n",
    "| my_sum | 8 | 19| 7 | 15| 7 | 13| 12| 14|\n",
    "\n",
    "This can be done with the following serial code (Pseudo-code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_sum = 0\n",
    "my_first_i = ...\n",
    "my_last_i = ...\n",
    "for i in range(my_first_i, my_last_i):\n",
    "    my_x = \"Compute next value\"\n",
    "    my_sum += my_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if \"I’m the master core\":\n",
    "    sum = my x\n",
    "    for \"each core other than myself\":\n",
    "        \"receive value from_core\"\n",
    "        sum += value\n",
    "else:\n",
    "    \"send my_x to the master\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When the cores are done computing their values of $my$ $sum$, they send their results to a designated $master$ core, which can form a $global$ $sum$. In our example, if the master core is $core$ $0$, it would add the values 8+19+7+15+7+13+12+14=95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can probably see another way to do this: instead of making the master core do all the work of computing the final sum, we can $pair$ the cores as shown below:\n",
    "\n",
    "<img src=\"images/Figure_1.png\" width=\"800\" height=\"800\" alt=\"Example\"  align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With $p=1000$ cores, the first method will require $999$ receives and adds, while the second will only require $10$, an improvement of almost a factor of $100!$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Conclusions</h3>\n",
    "\n",
    "- The **first global sum** is a fairly obvious generalization of the **serial global sum**.\n",
    "- The **second global sum**, on the other hand, bears little relation to the original serial addition.\n",
    "\n",
    "- The point here is that it’s unlikely that a **translation program** would “discover” the second global sum.\n",
    "\n",
    "- Thus, we cannot simply continue to write serial programs, we must write parallel programs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">How to Write Parallel Programs?</h3>\n",
    "\n",
    "$\\bullet$ There are a number of possible answers to this question, but most of them depend on the basic idea of <br>\n",
    "&ensp; $partitioning$ the work to be done among the cores. \n",
    "\n",
    "\n",
    "$\\bullet$ There are two widely used approaches:\n",
    "<br>\n",
    "&emsp; $\\bullet$ **task-parallelism**: partitioning the various tasks carried out in solving the problem among the cores.\n",
    "<br>\n",
    "&emsp; $\\bullet$ **data-parallelism**: partitioning the data used in solving the problem among the cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example 2</h3>\n",
    "\n",
    "Suppose that **Prof. E** has to teach a section **\"Distributed and Parallel Computing\"**.\n",
    "<br>\n",
    "Also suppose that **Prof. E** has **thirty students** in his section, \n",
    "<br>\n",
    "so he has been assigned a teaching assistants (TAs): **Dr. K** and **Dr. S**.\n",
    "<br>\n",
    "At last the semester is over, and **Prof. E** makes up a final exam that consists of **three questions**.\n",
    "<br>\n",
    "In order to grade the exam, **Prof. E** and his TAs might consider the following two options:\n",
    "- each of them can grade all thirty responses to one of the questions. <br> Say **Prof. P** grades question **1**, **Dr. K** grades question **2** and **Dr. S** grades question **3**;\n",
    "- they divide the **thirty** exams into **three piles** of **ten exams** each, <br>and each of them can grade all the papers in one of the piles.\n",
    "\n",
    "In both approaches the **cores** are the **Prof. E** and his TAs (**Dr. K** and **Dr. S**).\n",
    "<br>\n",
    "- The **first** approach might be considered an example of **task-parallelism**\n",
    "- The **second** approach might be considered an example of **data-parallelism**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Coordination of the Cores</h3>\n",
    "\n",
    "When we write parallel programs, we usually need to **coordinate** the work of the cores.\n",
    "<br>\n",
    "Let's show this on Example 1. In both global sum examples coordination involves:\n",
    "<br>\n",
    "&emsp; $\\bullet$ **communication** among the cores: one or more cores send their current partial sums to another core;\n",
    "<br>\n",
    "&emsp; $\\bullet$ **load balancing**: all cores are assigned roughly the same number of values to compute;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A third type of coordination is **synchronization**.\n",
    "<br>In most systems the cores are not automatically synchronized: each core works at its own pace.\n",
    "<br>\n",
    "Suppose that we need to **read data** from file. Say $x$ is an array that is read in by the **master** core:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if \"I’m the master core\":\n",
    "    fi = open('path/to/file.txt', 'r')\n",
    "    for i in range (0, n):\n",
    "        line = fi.readline()\n",
    "        x[i] = line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We don’t want the other cores to race ahead and start computing their partial sums before the master is done initializing $x$.\n",
    "Therefore, We need to add in a point of synchronization between the initialization of $x$ and the computation of the partial sums: *Synchronize cores()*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Shared and Distributed Memory</h3>\n",
    "\n",
    "There are two main types of parallel systems that we’ll be focusing on:\n",
    "<br>\n",
    "&emsp; $\\bullet$ **shared-Memory** systems (left): the cores share access to the computer’s memory;\n",
    "<br>\n",
    "&emsp; $\\bullet$ **distributed-Memory** systems (right): each core has its own (private) memory.\n",
    "<br>\n",
    "&emsp; The cores communicate explicitly by sending messages across a network.\n",
    "\n",
    "\n",
    "<img src=\"images/Memory_Organization.png\" width=\"1200\" height=\"80\" alt=\"Example\"  align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Concurrent, Parallel and Distributed Computings</h3>\n",
    "\n",
    "Although there isn’t complete agreement on the distinction between the terms parallel, distributed, and concurrent, many authors make the following distinctions:\n",
    "\n",
    "- **parallel computing**: runs multiple tasks simultaneously on cores that are physically close to each other and that either share the same memory or are connected by a very high-speed network;\n",
    "\n",
    "- **distributed computing**, tend to be more “loosely coupled.” The tasks may be executed by multiple computers that are separated by large distances, and the tasks themselves are often executed by programs that were created independently.\n",
    "\n",
    "- **concurrent computing**: multiple tasks can be in progress at any instant. For example, multitasking operating system is also concurrent, even when it is run on a machine with only one core, since multiple tasks can be in progress at any instant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Parallel hardware and software** have grown out of conventional **serial hardware and software**\n",
    "\n",
    "In order to better understand the current state of parallel systems, let’s begin with a brief look at a few aspects of **serial systems**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">The von Neumann architecture</h3>\n",
    "\n",
    "The *classical* von Neumann architecture consists of: \n",
    "$$\\textbf{Main memory, Central Processing Unit (CPU) and Interconnection}.$$\n",
    "\n",
    "<img src=\"images/Von_Neumann_Model.png\" width=\"500\" height=\"500\" alt=\"Example\"  align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Main memory** consists of a collection of **locations**, each of which is capable of storing both **instructions** and **data**:\n",
    " - Every **location** consists of an **address**, which is used to access the location and the **contents** of the location — the instructions or data stored in the location.\n",
    "\n",
    "- **CPU** is divided into a **control unit** and an **Arithmetic and Logic Unit (ALU)**:\n",
    " - The **control unit** is responsible for deciding **which** instructions in a program should be executed;\n",
    " - The **ALU** is responsible for executing the actual instructions.\n",
    "\n",
    "- **Interconections** is used to transfer instructions between the CPU and memory. This has traditionally been a **bus**, which consists of a collection of parallel wires and some hardware controlling access to the wires.\n",
    " - When data/instructions are transferred from memory to the CPU, we say the data/instructions are **read** from memory. \n",
    " - When data are transferred from the CPU to memory, we  say the data are **written** to memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">The von Neumann Bottleneck</h3>\n",
    "\n",
    "- In 2010 CPUs are capable of executing instructions more than **one hundred times** faster than they can read items from main memory. \n",
    "- This rises the so called the **Von Neuman Bottleneck Problem**.\n",
    "- Led to the **Modifications to the Von Neuman Model**.\n",
    "\n",
    "<img src=\"images/Factory.png\" width=\"1800\" height=\"600\" alt=\"Example\"  align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Processes, Multitasking, and Threads</h3>\n",
    "\n",
    "$\\bullet$ **Operation System (OS)**: the most important software on a computer. \n",
    "<br>\n",
    "&emsp; It manages the computer’s hardware and software resources.\n",
    "<br>\n",
    "$\\bullet$ **Multitasking**: most modern OS supports the simultaneous execution of **multiple programs**.\n",
    "<br>\n",
    "&emsp; This is **possible** even on a system with a **single core**.\n",
    "<br>\n",
    "$\\bullet$ **Process** - an instance of a computer program that is being executed. A process consists of:\n",
    "<br>\n",
    "&emsp; $\\bullet$ The executable machine language program.\n",
    "<br>\n",
    "&emsp; $\\bullet$ A block of memory, which will include the executable code.\n",
    "<br>\n",
    "&emsp; $\\bullet$ Descriptors of resources that the operating system has allocated to the process.\n",
    "<br>\n",
    "&emsp; $\\bullet$ Security information specifying which hardware and software resources can be used.\n",
    "<br>\n",
    "&emsp; $\\bullet$ Information about the state of the process (ready, registry, process' memory).\n",
    "<br>\n",
    "$\\bullet$ **Threading**: a mechanism to divide the process into more or less independent tasks.\n",
    "<br>\n",
    "&emsp; A **threads** can be stopped and started **much faster** than **processes**.\n",
    "<img src=\"images/Process.png\" width=\"800\" height=\"200\" alt=\"Example\"  align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Modification to the Von Neumann Model: CPU Caching</h3>\n",
    "\n",
    "\n",
    "$\\bullet$ **CPU cache** is a collection of memory locations that CPU can access more quickly than main memory.\n",
    "<br>\n",
    "$\\bullet$ **Caching** is one of the most widely used methods of addressing the von Neumann bottleneck:\n",
    "<br>\n",
    "&emsp; $\\bullet$ **widen** the road, i.e. transport more data or more instructions in a single memory access.\n",
    "<br>\n",
    "&emsp; $\\bullet$ **move** the **factory/warehouse**, i.e. store blocks of data and instructions in special memory, \n",
    "<br> \n",
    "&emsp; &emsp; that is effectively closer to the registers in the CPU.\n",
    "<br> \n",
    "$\\bullet$ **Locality**: the principle that an access of one location is followed by an access of a nearby location.\n",
    "<br> \n",
    "$\\bullet$ **Cache blocks (or cache lines)**: stores **8 to 16 times** as much information as a single memory.\n",
    "<br> \n",
    "$\\bullet$ **Cache levels**: the first level (**L1**) is the smallest**&**fastest, and higher levels (**L2**, . . . ) are larger**&**slower.\n",
    "<br> \n",
    "$\\bullet$ **Cache hit**: when a cache is checked for information and the information is available.\n",
    "<br> \n",
    "$\\bullet$ **Cache miss**: when a cache is checked for information and the information is unavailable.\n",
    "<br> \n",
    "$\\bullet$ **Inconsistecy**: the value in the cache and the value in main memory are different.\n",
    "<br>\n",
    "&emsp; $\\bullet$ **In write-through caches**: the line is written to main memory when it is written to the cache.\n",
    "<br>\n",
    "&emsp; $\\bullet$ **In write-back caches**: the line is written to main memory when the cache line is replaced by a new one.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Modification to the Von Neumann Model: Cache Mapping</h3>\n",
    "\n",
    "\n",
    "$\\bullet$ Another issue in cache design is deciding **where lines should be stored**:\n",
    "<br>\n",
    "&emsp; $\\bullet$ **fully associative cache**: a new line can be placed at any location in the cache.\n",
    "<br>\n",
    "&emsp; $\\bullet$ **direct mapped cahce**: each cache line has a unique location in the cache to which it will be assigned.\n",
    "<br>\n",
    "&emsp; $\\bullet$ $n$-**way set associative.**: each cache line can be placed in one of $n$ different locations in the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Suppose our **main memory** consists of $8$ lines and our **cache** consists of $4$ lines:\n",
    "\n",
    "|  Memory Index  | Fully Associative | Direct Mapped| 2-Way Set Associative|\n",
    "|----------------|-------------------|--------------|----------------------|\n",
    "|        0       | 0, 1, 2, or 3     |      0       |     0 or 1           |\n",
    "|        1       | 0, 1, 2, or 3     |      1       |     2 or 3           |\n",
    "|        2       | 0, 1, 2, or 3     |      2       |     0 or 1           |\n",
    "|        3       | 0, 1, 2, or 3     |      3       |     2 or 3           |\n",
    "|        4       | 0, 1, 2, or 3     |      0       |     0 or 1           |\n",
    "|        5       | 0, 1, 2, or 3     |      1       |     2 or 3           |\n",
    "|        6       | 0, 1, 2, or 3     |      2       |     0 or 1           |\n",
    "|        7       | 0, 1, 2, or 3     |      3       |     2 or 3           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example</h3>\n",
    "\n",
    "\n",
    "$\\bullet$ **Note:** the workings of the CPU cache are **controlled by the system hardware**,\n",
    "<br>\n",
    "$\\bullet$ However, knowing the **principle of locality allows** us to have some indirect control over caching.\n",
    "\n",
    "$\\bullet$ Suppose that we need to **sum up** the data in the **two-dimensional array** and \n",
    "<br>\n",
    "$\\bullet$ let’s also suppose that cache is **direct mapped** and it can only store $8$ elements, or $2$ **cache lines**\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A[n][n], x[n], y[n]\n",
    "\n",
    "# First pair of loops\n",
    "A[n][n], x[n], y[n]\n",
    "for i in range(0,n):\n",
    "    for j in range(0,n):\n",
    "        y[i] += A[i][j]*x[j]\n",
    "\n",
    "# Second pair of loops\n",
    "for j in range(0,n):\n",
    "    for i in range(0,n):\n",
    "        y[i] += A[i][j]*x[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\bullet$ First pair of loops results in 2 misses while the second pair of loops results in 16 misses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Modification to the Von Neumann Model: Virtual Memory</h3>\n",
    "\n",
    "$\\bullet$ **Issues with main memory**: \n",
    "<br>\n",
    "&emsp; $\\bullet$ all of the instructions and data may not fit into **main memory** when we run **very large programs**.\n",
    "<br>\n",
    "&emsp; $\\bullet$ data and instructions must be protected while sharing the **main memory** among the running programs.\n",
    "\n",
    "$\\bullet$ **Virtual memory** was developed so that main memory can function as a cache for secondary storage. It:\n",
    "<br>\n",
    "&emsp;  $\\bullet$ operates on relatively large blocks (from $4$ to $16$ kilobytes) of data and instructions, called **pages**.\n",
    "<br>\n",
    "&emsp;  $\\bullet$ uses **virtual addresses** instead of addressing the memory used by a program with physical addresses.\n",
    "<br> \n",
    "&emsp;  $\\bullet$ uses **page table** to translate the virtual address into a physical address.\n",
    "<img src=\"images/Virtual_Memory.jpg\" width=\"700\" height=\"400\" alt=\"Example\"  align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\">Modification to the Von Neumann Model: Instruction-Level Parallelism</h3>\n",
    "\n",
    "$\\bullet$ **Instruction-Level Parallelism (ILP)** is used to improve processor performance by having multiple processor components or **functional units** simultaneously executing instructions.\n",
    "There are two main approaches to ILP:\n",
    "<br>\n",
    "&emsp; $\\bullet$ **pipelining**, in which functional units are arranged in stages.\n",
    "<br>\n",
    "&emsp; $\\bullet$ **multiple issue**, in which multiple instructions can be simultaneously initiated.\n",
    "<br>\n",
    "$\\bullet$ Both approaches are used in virtually all modern CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\bullet$ In general, **ILP** can be **very difficult to exploit**: it is a program with a long sequence of dependent statements offers few opportunities.\n",
    "<br>\n",
    "$\\bullet$ For example, in a direct calculation of the **Fibonacci numbers** there’s essentially no opportunity for simultaneous execution of instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "F[0] = F[1] = 1\n",
    "for i in range(2, n):\n",
    "  F[i] = F[i-1] + F[i-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Pipelining</h3>\n",
    "\n",
    "$\\bullet$ The principle of pipelining is similar to a **factory assembly line**:\n",
    "<br>\n",
    "&emsp; $\\bullet$ the **first team** is bolting a car’s engine to the chassis.\n",
    "<br>\n",
    "&emsp; $\\bullet$ the **second team** can connect the transmission to the engine and the driveshaft of a car,\n",
    "<br>\n",
    "&emsp; &ensp; that’s already been processed by the first team.\n",
    "<br>\n",
    "&emsp; $\\bullet$ the **third team** can bolt the body to the chassis in a car that’s been processed by the first two teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Example</h3>\n",
    "\n",
    "$\\bullet$ Suppose we want to add two floating point numbers $3.14 \\cdot 10^{4}$ and $2.72 \\cdot 10^{3}$. The steps will be:\n",
    "\n",
    "<font size=\"5\">\n",
    "\n",
    "| Time  | Operation         | Operand 1            | Operand 2            | Result                  |\n",
    "|-------|-------------------|----------------------|----------------------|-------------------------|\n",
    "|   0   | Fetch operands    | $9.99 \\times 10^{9}$ | $8.88 \\times 10^{8}$ |                         |\n",
    "|   1   | Compare exponents | $9.99 \\times 10^{9}$ | $8.88 \\times 10^{8}$ |                         |\n",
    "|   2   | Shift one operand | $9.99 \\times 10^{9}$ | $8.88 \\times 10^{8}$ |                         |\n",
    "|   3   | Add               | $9.99 \\times 10^{9}$ | $8.88 \\times 10^{8}$ | $10.878 \\times 10^{9}$  |\n",
    "|   4   | Normalize result  | $9.99 \\times 10^{9}$ | $8.88 \\times 10^{8}$ | $1.0878 \\times 10^{10}$ |\n",
    "|   5   | Round result      | $9.99 \\times 10^{9}$ | $8.88 \\times 10^{8}$ | $1.09 \\times 10^{10}$   |\n",
    "|   6   | Store result      | $9.99 \\times 10^{9}$ | $8.88 \\times 10^{8}$ | $1.0878 \\times 10^{10}$ |\n",
    "|-------|-------------------|----------------------|----------------------|-------------------------|\n",
    "\n",
    "</font>\n",
    "\n",
    "$\\bullet$ If each of the operations takes one nanosecond, the addition operation will take 7 nanoseconds\n",
    "<br>\n",
    "$\\bullet$ So if we execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x[1000], y[1000], z[1000]\n",
    "...\n",
    "for i in range(0, 1000):\n",
    "    z[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "the **for** loop will take something like 7000 nanoseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\bullet$ Alternative  is to divide our floating point adder into seven separate pieces and use  the **pipelining**:\n",
    "\n",
    "| Time  | Fetch | Compare | Shift | Add | Normalize | Roound | Store |\n",
    "|-------|-------|---------|-------|-----|-----------|--------|-------|\n",
    "| 0     | 0     |         |       |     |           |        |       |\n",
    "| 1     | 1     | 0       |       |     |           |        |       |\n",
    "| 2     | 2     | 1       | 0     |     |           |        |       |\n",
    "| 3     | 3     | 2       | 1     | 0   |           |        |       |\n",
    "| 4     | 4     | 3       | 2     | 1   | 0         |        |       |\n",
    "| 5     | 5     | 4       | 3     | 2   | 1         | 0      |       |\n",
    "| 6     | 6     | 5       | 4     | 3   | 2         | 1      | 0     |\n",
    "| ...   | ...   | ...     | ...   | ... | ...       | ...    | ...   |\n",
    "| 999   | 999   | 998     | 997   | 996 | 995       | 994    | 993   |\n",
    "| 1000  |       | 999     | 998   | 997 | 996       | 995    | 994   |\n",
    "| 1001  |       |         | 999   | 998 | 997       | 996    | 995   |\n",
    "| 1002  |       |         |       | 999 | 998       | 997    | 996   |\n",
    "| 1003  |       |         |       |     | 999       | 998    | 997   |\n",
    "| 1004  |       |         |       |     |           | 999    | 998   |\n",
    "| 1005  |       |         |       |     |           |        | 999   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Multiple Issue</h3>\n",
    "\n",
    "$\\bullet$ **Multiple issue** processors replicate functional units and try to simultaneously execute different instructions in a program.\n",
    "<br>\n",
    "$\\bullet$ For example, we can approximately **halve the time** of the **floating point adders** to execute the loop: \n",
    "<br>\n",
    "&emsp; $\\bullet$  while the first adder is computing $z[0]$, the second can compute $z[1]$; \n",
    "<br> \n",
    "&emsp; $\\bullet$  while the first is computing $z[2]$, the second can compute $z[3]$; and so on.\n",
    "<br>\n",
    "$\\bullet$ **Static** multiple issue: the functional units are scheduled at compile time\n",
    "<br>\n",
    "$\\bullet$ **Dynamic** multiple issue: the functional units scheduled at run-time.\n",
    "\n",
    "$\\bullet$ A **processor** that supports **dynamic** multiple issue is sometimes said to be **superscalar**.\n",
    "<br>\n",
    "$\\bullet$ The most important techniques to find instructions that can be executed simultaneously is **speculation**:\n",
    "<br>\n",
    "&ensp; the processor makes a guess about an instruction, then executes the instruction on the basis of the guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "z = x + y\n",
    "if (z > 0): w = x\n",
    "    \n",
    "else:\n",
    "    w = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align=\"center\">Modification to the Von Neumann Model: Thread-Level Parallelism</h3>\n",
    "\n",
    "$\\bullet$ **Thread-level parallelism (TLP)** attempts to provide parallelism through the simultaneous execution of different threads, i.e. it's more **coarser-grained** parallelism rather than the **finer-grained** parallelism.\n",
    "\n",
    "$\\bullet$ In **fine-grained** multithreading, the processor switches between threads after each instruction, skipping threads that are stalled. While this approach has the potential to avoid wasted machine time due to stalls, it has the drawback that a thread that’s ready to execute a long sequence of instructions may have to wait to execute every instruction.\n",
    "\n",
    "$\\bullet$ **Coarse-grained** multithreading attempts to avoid this problem by only switching threads that are stalled waiting for a time-consuming operation to complete.\n",
    "\n",
    "$\\bullet$  In order for **TLP** to be useful, the system must support **very rapid switching** between threads.\n",
    "\n",
    "$\\bullet$ **Simultaneous multithreading (SMT)** is a variation on fine-grained multithreading. It attempts to exploit superscalar processors by allowing multiple threads."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
